{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to genereate possible ensembles results based on previosuly stored scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows you to compute the optimal ensemble (in terms of average AUC-ROC) based on any algorithms you wish to include. However, it is important to note that the scores related to that algorithm must be computed and stored first. This can be done by either running the gridsearch, allowing you to deduce the optimal parameters for an algorithm and obtain the associated scores to each dataset for that set of parameters, or by running get_default_scores, allowing you to get the scores for the default parameters of that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from collections import defaultdict\n",
    "import traceback\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that evaluates the algorithm based on the produced anomaly scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(scores, data_path):\n",
    "    # Load test data\n",
    "    data_test = pd.read_csv(data_path)\n",
    "    \n",
    "    # Ensure 'is_anomaly' column exists in the data\n",
    "    if 'is_anomaly' not in data_test.columns:\n",
    "        raise ValueError(f\"The test data at {data_path} must contain an 'is_anomaly' column.\")\n",
    "\n",
    "    # Extract the anomaly labels\n",
    "    anomalies = data_test['is_anomaly']\n",
    "\n",
    "    # Calculate AUC-ROC and AUC-PR\n",
    "    auc_roc = roc_auc_score(anomalies, scores)\n",
    "    precision, recall, _ = precision_recall_curve(anomalies, scores)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    return auc_roc, auc_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General function allowing you to compute the best ensemble in terms of average AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_algorithms(algorithm_names, family=\"distance\", n_weight_points=11):\n",
    "    # Load scores for each algorithm upfront\n",
    "    algorithm_scores = {}\n",
    "    for algo in algorithm_names:\n",
    "        try:\n",
    "            # Load scores\n",
    "            try:\n",
    "                # Attempt to load the primary file, containing the best scores\n",
    "                file_path = f'./results/best_{algo.lower()}_scores.npz' # Very important that your scores are saved in this format if you have already performed grid search!\n",
    "                if not os.path.exists(file_path):\n",
    "                    # If the primary file doesn't exist, switch to the default file\n",
    "                    file_path = f'./results/default_{algo.lower()}_scores.npz' # Very important that your scores are saved in this format if you have already performed default score calculation!\n",
    "                    if not os.path.exists(file_path):\n",
    "                        raise FileNotFoundError(f\"No stored scores found for algorithm {algo} in either file.\")\n",
    "                \n",
    "                # Load the scores and convert to a proper dictionary\n",
    "                scores = np.load(file_path)\n",
    "                algorithm_scores[algo] = dict(scores)\n",
    "            except FileNotFoundError as e:\n",
    "                raise FileNotFoundError(str(e))\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    # Generate weight combinations\n",
    "    n_algorithms = len(algorithm_names)\n",
    "    weight_points = np.linspace(0, 1, n_weight_points)\n",
    "\n",
    "    def generate_weight_combinations(n_algos):\n",
    "        if n_algos == 2:\n",
    "            return np.array([[w, 1 - w] for w in weight_points])\n",
    "\n",
    "        weights = []\n",
    "        for w in weight_points:\n",
    "            sub_weights = generate_weight_combinations(n_algos - 1)\n",
    "            for sw in sub_weights:\n",
    "                if sum(sw) <= 1:\n",
    "                    weights.append([w] + list((1 - w) * sw / sum(sw)))\n",
    "        return np.array(weights)\n",
    "\n",
    "    weight_combinations = generate_weight_combinations(n_algorithms)\n",
    "\n",
    "    # Track the best results\n",
    "    best_mean_auc_roc = -1\n",
    "    best_results = {\n",
    "        'weights': None,\n",
    "        'mean_auc_roc': -1,\n",
    "        'dataset_scores': {},\n",
    "        'metrics': {}\n",
    "    }\n",
    "\n",
    "    # Process datasets for all weight combinations\n",
    "    datasets = list(algorithm_scores[algorithm_names[0]].keys())\n",
    "    for weights in weight_combinations:\n",
    "        print(f\"Testing weights: {weights}\")\n",
    "        total_auc_roc = 0\n",
    "        current_results = {\n",
    "            'dataset_scores': {},\n",
    "            'metrics': defaultdict(dict)\n",
    "        }\n",
    "\n",
    "        for dataset in datasets:\n",
    "            try:\n",
    "                # Load dataset and labels\n",
    "                data_test = pd.read_csv(dataset)\n",
    "                if data_test is None:\n",
    "                    print(f\"Skipping file {dataset} due to loading error\")\n",
    "                    continue\n",
    "                \n",
    "                true_labels = data_test['is_anomaly'].values\n",
    "\n",
    "                # Get scores for this dataset from each algorithm\n",
    "                algo_scores = []\n",
    "                for algo in algorithm_names:\n",
    "                    scores = algorithm_scores[algo][dataset] if dataset in algorithm_scores[algo] else np.zeros(len(true_labels))\n",
    "                    min_val, max_val = np.min(scores), np.max(scores)\n",
    "                    if min_val != max_val:\n",
    "                        scores = (scores - min_val) / (max_val - min_val)\n",
    "                    algo_scores.append(scores)\n",
    "\n",
    "                # Compute the weighted sum of scores\n",
    "                combined_scores = np.zeros_like(algo_scores[0])\n",
    "                for i, w in enumerate(weights):\n",
    "                    combined_scores += w * algo_scores[i]\n",
    "\n",
    "                # Evaluate and store results\n",
    "                auc_roc, auc_pr = evaluate_algorithm(combined_scores, dataset)\n",
    "                current_results['dataset_scores'][dataset] = combined_scores\n",
    "                current_results['metrics'][dataset] = {\n",
    "                    'AUC-ROC': auc_roc,\n",
    "                    'AUC-PR': auc_pr\n",
    "                }\n",
    "                total_auc_roc += auc_roc\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing dataset {dataset}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        # Calculate mean AUC-ROC for current weight set\n",
    "        current_mean_auc_roc = total_auc_roc / len(datasets)\n",
    "\n",
    "        print(f\"Mean AUC-ROC for weights {weights}: {current_mean_auc_roc:.3f}\")\n",
    "\n",
    "        # Update best results if current results are better\n",
    "        if current_mean_auc_roc > best_mean_auc_roc:\n",
    "            print(f\"New best configuration found! Previous best: {best_mean_auc_roc:.3f}\")\n",
    "            best_mean_auc_roc = current_mean_auc_roc\n",
    "            best_results = {\n",
    "                'weights': weights,\n",
    "                'mean_auc_roc': current_mean_auc_roc,\n",
    "                'dataset_scores': current_results['dataset_scores'].copy(),\n",
    "                'metrics': dict(current_results['metrics'])\n",
    "            }\n",
    "\n",
    "        # Clear memory\n",
    "        gc.collect()\n",
    "\n",
    "    # Save final results\n",
    "    try:\n",
    "        os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "        # Save weights and metrics\n",
    "        weights_df = pd.DataFrame({\n",
    "            'Algorithm': algorithm_names,\n",
    "            'Weight': best_results['weights']\n",
    "        })\n",
    "        weights_df.to_csv(f'./results/best_algorithm_combination_{family}_weights.csv', index=False)\n",
    "\n",
    "        # Save detailed metrics for each dataset\n",
    "        detailed_metrics = []\n",
    "        for dataset, metrics in best_results['metrics'].items():\n",
    "            detailed_metrics.append({\n",
    "                'Dataset': dataset,\n",
    "                'AUC-ROC': metrics['AUC-ROC'],\n",
    "                'AUC-PR': metrics['AUC-PR']\n",
    "            })\n",
    "        pd.DataFrame(detailed_metrics).to_csv(\n",
    "            f'./results/best_algorithm_combination_{family}_metrics.csv', index=False\n",
    "        )\n",
    "\n",
    "        # Save anomaly scores for each dataset\n",
    "        np.savez(\n",
    "            f'./results/best_algorithm_combination_{family}_scores.npz',\n",
    "            **best_results['dataset_scores']\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of ensemble for 3 distance-based methods: Sub-LOF, kMeans & PS-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing weights: [0. 0. 1.]\n",
      "Mean AUC-ROC for weights [0. 0. 1.]: 0.679\n",
      "New best configuration found! Previous best: -1.000\n",
      "Testing weights: [0.  0.1 0.9]\n",
      "Mean AUC-ROC for weights [0.  0.1 0.9]: 0.710\n",
      "New best configuration found! Previous best: 0.679\n",
      "Testing weights: [0.  0.2 0.8]\n",
      "Mean AUC-ROC for weights [0.  0.2 0.8]: 0.725\n",
      "New best configuration found! Previous best: 0.710\n",
      "Testing weights: [0.  0.3 0.7]\n",
      "Mean AUC-ROC for weights [0.  0.3 0.7]: 0.738\n",
      "New best configuration found! Previous best: 0.725\n",
      "Testing weights: [0.  0.4 0.6]\n",
      "Mean AUC-ROC for weights [0.  0.4 0.6]: 0.748\n",
      "New best configuration found! Previous best: 0.738\n",
      "Testing weights: [0.  0.5 0.5]\n",
      "Mean AUC-ROC for weights [0.  0.5 0.5]: 0.757\n",
      "New best configuration found! Previous best: 0.748\n",
      "Testing weights: [0.  0.6 0.4]\n",
      "Mean AUC-ROC for weights [0.  0.6 0.4]: 0.764\n",
      "New best configuration found! Previous best: 0.757\n",
      "Testing weights: [0.  0.7 0.3]\n",
      "Mean AUC-ROC for weights [0.  0.7 0.3]: 0.766\n",
      "New best configuration found! Previous best: 0.764\n",
      "Testing weights: [0.  0.8 0.2]\n",
      "Mean AUC-ROC for weights [0.  0.8 0.2]: 0.763\n",
      "Testing weights: [0.  0.9 0.1]\n",
      "Mean AUC-ROC for weights [0.  0.9 0.1]: 0.760\n",
      "Testing weights: [0. 1. 0.]\n",
      "Mean AUC-ROC for weights [0. 1. 0.]: 0.750\n",
      "Testing weights: [0.1 0.  0.9]\n",
      "Mean AUC-ROC for weights [0.1 0.  0.9]: 0.723\n",
      "Testing weights: [0.1  0.09 0.81]\n",
      "Mean AUC-ROC for weights [0.1  0.09 0.81]: 0.739\n",
      "Testing weights: [0.1  0.18 0.72]\n",
      "Mean AUC-ROC for weights [0.1  0.18 0.72]: 0.749\n",
      "Testing weights: [0.1  0.27 0.63]\n",
      "Mean AUC-ROC for weights [0.1  0.27 0.63]: 0.757\n",
      "Testing weights: [0.1  0.36 0.54]\n",
      "Mean AUC-ROC for weights [0.1  0.36 0.54]: 0.764\n",
      "Testing weights: [0.1  0.45 0.45]\n",
      "Mean AUC-ROC for weights [0.1  0.45 0.45]: 0.771\n",
      "New best configuration found! Previous best: 0.766\n",
      "Testing weights: [0.1  0.54 0.36]\n",
      "Mean AUC-ROC for weights [0.1  0.54 0.36]: 0.776\n",
      "New best configuration found! Previous best: 0.771\n",
      "Testing weights: [0.1  0.63 0.27]\n",
      "Mean AUC-ROC for weights [0.1  0.63 0.27]: 0.778\n",
      "New best configuration found! Previous best: 0.776\n",
      "Testing weights: [0.1  0.72 0.18]\n",
      "Mean AUC-ROC for weights [0.1  0.72 0.18]: 0.774\n",
      "Testing weights: [0.1  0.81 0.09]\n",
      "Mean AUC-ROC for weights [0.1  0.81 0.09]: 0.771\n",
      "Testing weights: [0.1 0.9 0. ]\n",
      "Mean AUC-ROC for weights [0.1 0.9 0. ]: 0.765\n",
      "Testing weights: [0.2 0.  0.8]\n",
      "Mean AUC-ROC for weights [0.2 0.  0.8]: 0.745\n",
      "Testing weights: [0.2  0.08 0.72]\n",
      "Mean AUC-ROC for weights [0.2  0.08 0.72]: 0.755\n",
      "Testing weights: [0.2  0.16 0.64]\n",
      "Mean AUC-ROC for weights [0.2  0.16 0.64]: 0.763\n",
      "Testing weights: [0.2  0.24 0.56]\n",
      "Mean AUC-ROC for weights [0.2  0.24 0.56]: 0.770\n",
      "Testing weights: [0.2  0.32 0.48]\n",
      "Mean AUC-ROC for weights [0.2  0.32 0.48]: 0.776\n",
      "Testing weights: [0.2 0.4 0.4]\n",
      "Mean AUC-ROC for weights [0.2 0.4 0.4]: 0.782\n",
      "New best configuration found! Previous best: 0.778\n",
      "Testing weights: [0.2  0.48 0.32]\n",
      "Mean AUC-ROC for weights [0.2  0.48 0.32]: 0.786\n",
      "New best configuration found! Previous best: 0.782\n",
      "Testing weights: [0.2  0.56 0.24]\n",
      "Mean AUC-ROC for weights [0.2  0.56 0.24]: 0.786\n",
      "New best configuration found! Previous best: 0.786\n",
      "Testing weights: [0.2  0.64 0.16]\n",
      "Mean AUC-ROC for weights [0.2  0.64 0.16]: 0.783\n",
      "Testing weights: [0.2  0.72 0.08]\n",
      "Mean AUC-ROC for weights [0.2  0.72 0.08]: 0.780\n",
      "Testing weights: [0.2 0.8 0. ]\n",
      "Mean AUC-ROC for weights [0.2 0.8 0. ]: 0.775\n",
      "Testing weights: [0.3 0.  0.7]\n",
      "Mean AUC-ROC for weights [0.3 0.  0.7]: 0.758\n",
      "Testing weights: [0.3  0.07 0.63]\n",
      "Mean AUC-ROC for weights [0.3  0.07 0.63]: 0.766\n",
      "Testing weights: [0.3  0.14 0.56]\n",
      "Mean AUC-ROC for weights [0.3  0.14 0.56]: 0.773\n",
      "Testing weights: [0.3  0.21 0.49]\n",
      "Mean AUC-ROC for weights [0.3  0.21 0.49]: 0.779\n",
      "Testing weights: [0.3  0.28 0.42]\n",
      "Mean AUC-ROC for weights [0.3  0.28 0.42]: 0.785\n",
      "Testing weights: [0.3  0.35 0.35]\n",
      "Mean AUC-ROC for weights [0.3  0.35 0.35]: 0.790\n",
      "New best configuration found! Previous best: 0.786\n",
      "Testing weights: [0.3  0.42 0.28]\n",
      "Mean AUC-ROC for weights [0.3  0.42 0.28]: 0.792\n",
      "New best configuration found! Previous best: 0.790\n",
      "Testing weights: [0.3  0.49 0.21]\n",
      "Mean AUC-ROC for weights [0.3  0.49 0.21]: 0.792\n",
      "Testing weights: [0.3  0.56 0.14]\n",
      "Mean AUC-ROC for weights [0.3  0.56 0.14]: 0.789\n",
      "Testing weights: [0.3  0.63 0.07]\n",
      "Mean AUC-ROC for weights [0.3  0.63 0.07]: 0.787\n",
      "Testing weights: [0.3 0.7 0. ]\n",
      "Mean AUC-ROC for weights [0.3 0.7 0. ]: 0.782\n",
      "Testing weights: [0.4 0.  0.6]\n",
      "Mean AUC-ROC for weights [0.4 0.  0.6]: 0.767\n",
      "Testing weights: [0.4  0.06 0.54]\n",
      "Mean AUC-ROC for weights [0.4  0.06 0.54]: 0.773\n",
      "Testing weights: [0.4  0.12 0.48]\n",
      "Mean AUC-ROC for weights [0.4  0.12 0.48]: 0.780\n",
      "Testing weights: [0.4  0.18 0.42]\n",
      "Mean AUC-ROC for weights [0.4  0.18 0.42]: 0.786\n",
      "Testing weights: [0.4  0.24 0.36]\n",
      "Mean AUC-ROC for weights [0.4  0.24 0.36]: 0.791\n",
      "Testing weights: [0.4 0.3 0.3]\n",
      "Mean AUC-ROC for weights [0.4 0.3 0.3]: 0.795\n",
      "New best configuration found! Previous best: 0.792\n",
      "Testing weights: [0.4  0.36 0.24]\n",
      "Mean AUC-ROC for weights [0.4  0.36 0.24]: 0.797\n",
      "New best configuration found! Previous best: 0.795\n",
      "Testing weights: [0.4  0.42 0.18]\n",
      "Mean AUC-ROC for weights [0.4  0.42 0.18]: 0.796\n",
      "Testing weights: [0.4  0.48 0.12]\n",
      "Mean AUC-ROC for weights [0.4  0.48 0.12]: 0.793\n",
      "Testing weights: [0.4  0.54 0.06]\n",
      "Mean AUC-ROC for weights [0.4  0.54 0.06]: 0.791\n",
      "Testing weights: [0.4 0.6 0. ]\n",
      "Mean AUC-ROC for weights [0.4 0.6 0. ]: 0.787\n",
      "Testing weights: [0.5 0.  0.5]\n",
      "Mean AUC-ROC for weights [0.5 0.  0.5]: 0.774\n",
      "Testing weights: [0.5  0.05 0.45]\n",
      "Mean AUC-ROC for weights [0.5  0.05 0.45]: 0.782\n",
      "Testing weights: [0.5 0.1 0.4]\n",
      "Mean AUC-ROC for weights [0.5 0.1 0.4]: 0.786\n",
      "Testing weights: [0.5  0.15 0.35]\n",
      "Mean AUC-ROC for weights [0.5  0.15 0.35]: 0.790\n",
      "Testing weights: [0.5 0.2 0.3]\n",
      "Mean AUC-ROC for weights [0.5 0.2 0.3]: 0.793\n",
      "Testing weights: [0.5  0.25 0.25]\n",
      "Mean AUC-ROC for weights [0.5  0.25 0.25]: 0.797\n",
      "New best configuration found! Previous best: 0.797\n",
      "Testing weights: [0.5 0.3 0.2]\n",
      "Mean AUC-ROC for weights [0.5 0.3 0.2]: 0.798\n",
      "New best configuration found! Previous best: 0.797\n",
      "Testing weights: [0.5  0.35 0.15]\n",
      "Mean AUC-ROC for weights [0.5  0.35 0.15]: 0.797\n",
      "Testing weights: [0.5 0.4 0.1]\n",
      "Mean AUC-ROC for weights [0.5 0.4 0.1]: 0.795\n",
      "Testing weights: [0.5  0.45 0.05]\n",
      "Mean AUC-ROC for weights [0.5  0.45 0.05]: 0.793\n",
      "Testing weights: [0.5 0.5 0. ]\n",
      "Mean AUC-ROC for weights [0.5 0.5 0. ]: 0.790\n",
      "Testing weights: [0.6 0.  0.4]\n",
      "Mean AUC-ROC for weights [0.6 0.  0.4]: 0.783\n",
      "Testing weights: [0.6  0.04 0.36]\n",
      "Mean AUC-ROC for weights [0.6  0.04 0.36]: 0.785\n",
      "Testing weights: [0.6  0.08 0.32]\n",
      "Mean AUC-ROC for weights [0.6  0.08 0.32]: 0.788\n",
      "Testing weights: [0.6  0.12 0.28]\n",
      "Mean AUC-ROC for weights [0.6  0.12 0.28]: 0.790\n",
      "Testing weights: [0.6  0.16 0.24]\n",
      "Mean AUC-ROC for weights [0.6  0.16 0.24]: 0.794\n",
      "Testing weights: [0.6 0.2 0.2]\n",
      "Mean AUC-ROC for weights [0.6 0.2 0.2]: 0.796\n",
      "Testing weights: [0.6  0.24 0.16]\n",
      "Mean AUC-ROC for weights [0.6  0.24 0.16]: 0.797\n",
      "Testing weights: [0.6  0.28 0.12]\n",
      "Mean AUC-ROC for weights [0.6  0.28 0.12]: 0.796\n",
      "Testing weights: [0.6  0.32 0.08]\n",
      "Mean AUC-ROC for weights [0.6  0.32 0.08]: 0.794\n",
      "Testing weights: [0.6  0.36 0.04]\n",
      "Mean AUC-ROC for weights [0.6  0.36 0.04]: 0.793\n",
      "Testing weights: [0.6 0.4 0. ]\n",
      "Mean AUC-ROC for weights [0.6 0.4 0. ]: 0.791\n",
      "Testing weights: [0.7 0.  0.3]\n",
      "Mean AUC-ROC for weights [0.7 0.  0.3]: 0.784\n",
      "Testing weights: [0.7  0.03 0.27]\n",
      "Mean AUC-ROC for weights [0.7  0.03 0.27]: 0.786\n",
      "Testing weights: [0.7  0.06 0.24]\n",
      "Mean AUC-ROC for weights [0.7  0.06 0.24]: 0.788\n",
      "Testing weights: [0.7  0.09 0.21]\n",
      "Mean AUC-ROC for weights [0.7  0.09 0.21]: 0.790\n",
      "Testing weights: [0.7  0.12 0.18]\n",
      "Mean AUC-ROC for weights [0.7  0.12 0.18]: 0.792\n",
      "Testing weights: [0.7  0.15 0.15]\n",
      "Mean AUC-ROC for weights [0.7  0.15 0.15]: 0.794\n",
      "Testing weights: [0.7  0.18 0.12]\n",
      "Mean AUC-ROC for weights [0.7  0.18 0.12]: 0.795\n",
      "Testing weights: [0.7  0.21 0.09]\n",
      "Mean AUC-ROC for weights [0.7  0.21 0.09]: 0.794\n",
      "Testing weights: [0.7  0.24 0.06]\n",
      "Mean AUC-ROC for weights [0.7  0.24 0.06]: 0.792\n",
      "Testing weights: [0.7  0.27 0.03]\n",
      "Mean AUC-ROC for weights [0.7  0.27 0.03]: 0.792\n",
      "Testing weights: [0.7 0.3 0. ]\n",
      "Mean AUC-ROC for weights [0.7 0.3 0. ]: 0.790\n",
      "Testing weights: [0.8 0.  0.2]\n",
      "Mean AUC-ROC for weights [0.8 0.  0.2]: 0.784\n",
      "Testing weights: [0.8  0.02 0.18]\n",
      "Mean AUC-ROC for weights [0.8  0.02 0.18]: 0.786\n",
      "Testing weights: [0.8  0.04 0.16]\n",
      "Mean AUC-ROC for weights [0.8  0.04 0.16]: 0.787\n",
      "Testing weights: [0.8  0.06 0.14]\n",
      "Mean AUC-ROC for weights [0.8  0.06 0.14]: 0.788\n",
      "Testing weights: [0.8  0.08 0.12]\n",
      "Mean AUC-ROC for weights [0.8  0.08 0.12]: 0.790\n",
      "Testing weights: [0.8 0.1 0.1]\n",
      "Mean AUC-ROC for weights [0.8 0.1 0.1]: 0.791\n",
      "Testing weights: [0.8  0.12 0.08]\n",
      "Mean AUC-ROC for weights [0.8  0.12 0.08]: 0.791\n",
      "Testing weights: [0.8  0.14 0.06]\n",
      "Mean AUC-ROC for weights [0.8  0.14 0.06]: 0.791\n",
      "Testing weights: [0.8  0.16 0.04]\n",
      "Mean AUC-ROC for weights [0.8  0.16 0.04]: 0.789\n",
      "Testing weights: [0.8  0.18 0.02]\n",
      "Mean AUC-ROC for weights [0.8  0.18 0.02]: 0.788\n",
      "Testing weights: [0.8 0.2 0. ]\n",
      "Mean AUC-ROC for weights [0.8 0.2 0. ]: 0.787\n",
      "Testing weights: [0.9 0.  0.1]\n",
      "Mean AUC-ROC for weights [0.9 0.  0.1]: 0.782\n",
      "Testing weights: [0.9  0.01 0.09]\n",
      "Mean AUC-ROC for weights [0.9  0.01 0.09]: 0.783\n",
      "Testing weights: [0.9  0.02 0.08]\n",
      "Mean AUC-ROC for weights [0.9  0.02 0.08]: 0.784\n",
      "Testing weights: [0.9  0.03 0.07]\n",
      "Mean AUC-ROC for weights [0.9  0.03 0.07]: 0.785\n",
      "Testing weights: [0.9  0.04 0.06]\n",
      "Mean AUC-ROC for weights [0.9  0.04 0.06]: 0.785\n",
      "Testing weights: [0.9  0.05 0.05]\n",
      "Mean AUC-ROC for weights [0.9  0.05 0.05]: 0.786\n",
      "Testing weights: [0.9  0.06 0.04]\n",
      "Mean AUC-ROC for weights [0.9  0.06 0.04]: 0.786\n",
      "Testing weights: [0.9  0.07 0.03]\n",
      "Mean AUC-ROC for weights [0.9  0.07 0.03]: 0.785\n",
      "Testing weights: [0.9  0.08 0.02]\n",
      "Mean AUC-ROC for weights [0.9  0.08 0.02]: 0.783\n",
      "Testing weights: [0.9  0.09 0.01]\n",
      "Mean AUC-ROC for weights [0.9  0.09 0.01]: 0.783\n",
      "Testing weights: [0.9 0.1 0. ]\n",
      "Mean AUC-ROC for weights [0.9 0.1 0. ]: 0.782\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "\n",
      "Best combination found:\n",
      "Sub-LOF: 0.500\n",
      "kMeans: 0.300\n",
      "PS-SVM: 0.200\n",
      "Mean AUC-ROC: 0.798\n"
     ]
    }
   ],
   "source": [
    "algorithms = [\"Sub-LOF\", \"kMeans\", \"PS-SVM\"]\n",
    "distance_results = combine_algorithms(algorithms, n_weight_points=11)\n",
    "print(\"\\nBest combination found:\")\n",
    "for algo, weight in zip(algorithms, distance_results['weights']):\n",
    "    print(f\"{algo}: {weight:.3f}\")\n",
    "print(f\"Mean AUC-ROC: {distance_results['mean_auc_roc']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of ensemble for 3 methods from different families: Sub-LOF, GrammarViz3 & DWT-MLEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing weights: [0. 0. 1.]\n",
      "Mean AUC-ROC for weights [0. 0. 1.]: 0.760\n",
      "New best configuration found! Previous best: -1.000\n",
      "Testing weights: [0.  0.1 0.9]\n",
      "Mean AUC-ROC for weights [0.  0.1 0.9]: 0.788\n",
      "New best configuration found! Previous best: 0.760\n",
      "Testing weights: [0.  0.2 0.8]\n",
      "Mean AUC-ROC for weights [0.  0.2 0.8]: 0.790\n",
      "New best configuration found! Previous best: 0.788\n",
      "Testing weights: [0.  0.3 0.7]\n",
      "Mean AUC-ROC for weights [0.  0.3 0.7]: 0.787\n",
      "Testing weights: [0.  0.4 0.6]\n",
      "Mean AUC-ROC for weights [0.  0.4 0.6]: 0.782\n",
      "Testing weights: [0.  0.5 0.5]\n",
      "Mean AUC-ROC for weights [0.  0.5 0.5]: 0.779\n",
      "Testing weights: [0.  0.6 0.4]\n",
      "Mean AUC-ROC for weights [0.  0.6 0.4]: 0.778\n",
      "Testing weights: [0.  0.7 0.3]\n",
      "Mean AUC-ROC for weights [0.  0.7 0.3]: 0.776\n",
      "Testing weights: [0.  0.8 0.2]\n",
      "Mean AUC-ROC for weights [0.  0.8 0.2]: 0.772\n",
      "Testing weights: [0.  0.9 0.1]\n",
      "Mean AUC-ROC for weights [0.  0.9 0.1]: 0.766\n",
      "Testing weights: [0. 1. 0.]\n",
      "Mean AUC-ROC for weights [0. 1. 0.]: 0.751\n",
      "Testing weights: [0.1 0.  0.9]\n",
      "Mean AUC-ROC for weights [0.1 0.  0.9]: 0.797\n",
      "New best configuration found! Previous best: 0.790\n",
      "Testing weights: [0.1  0.09 0.81]\n",
      "Mean AUC-ROC for weights [0.1  0.09 0.81]: 0.802\n",
      "New best configuration found! Previous best: 0.797\n",
      "Testing weights: [0.1  0.18 0.72]\n",
      "Mean AUC-ROC for weights [0.1  0.18 0.72]: 0.806\n",
      "New best configuration found! Previous best: 0.802\n",
      "Testing weights: [0.1  0.27 0.63]\n",
      "Mean AUC-ROC for weights [0.1  0.27 0.63]: 0.805\n",
      "Testing weights: [0.1  0.36 0.54]\n",
      "Mean AUC-ROC for weights [0.1  0.36 0.54]: 0.803\n",
      "Testing weights: [0.1  0.45 0.45]\n",
      "Mean AUC-ROC for weights [0.1  0.45 0.45]: 0.791\n",
      "Testing weights: [0.1  0.54 0.36]\n",
      "Mean AUC-ROC for weights [0.1  0.54 0.36]: 0.790\n",
      "Testing weights: [0.1  0.63 0.27]\n",
      "Mean AUC-ROC for weights [0.1  0.63 0.27]: 0.788\n",
      "Testing weights: [0.1  0.72 0.18]\n",
      "Mean AUC-ROC for weights [0.1  0.72 0.18]: 0.781\n",
      "Testing weights: [0.1  0.81 0.09]\n",
      "Mean AUC-ROC for weights [0.1  0.81 0.09]: 0.774\n",
      "Testing weights: [0.1 0.9 0. ]\n",
      "Mean AUC-ROC for weights [0.1 0.9 0. ]: 0.767\n",
      "Testing weights: [0.2 0.  0.8]\n",
      "Mean AUC-ROC for weights [0.2 0.  0.8]: 0.806\n",
      "Testing weights: [0.2  0.08 0.72]\n",
      "Mean AUC-ROC for weights [0.2  0.08 0.72]: 0.813\n",
      "New best configuration found! Previous best: 0.806\n",
      "Testing weights: [0.2  0.16 0.64]\n",
      "Mean AUC-ROC for weights [0.2  0.16 0.64]: 0.818\n",
      "New best configuration found! Previous best: 0.813\n",
      "Testing weights: [0.2  0.24 0.56]\n",
      "Mean AUC-ROC for weights [0.2  0.24 0.56]: 0.819\n",
      "New best configuration found! Previous best: 0.818\n",
      "Testing weights: [0.2  0.32 0.48]\n",
      "Mean AUC-ROC for weights [0.2  0.32 0.48]: 0.816\n",
      "Testing weights: [0.2 0.4 0.4]\n",
      "Mean AUC-ROC for weights [0.2 0.4 0.4]: 0.803\n",
      "Testing weights: [0.2  0.48 0.32]\n",
      "Mean AUC-ROC for weights [0.2  0.48 0.32]: 0.797\n",
      "Testing weights: [0.2  0.56 0.24]\n",
      "Mean AUC-ROC for weights [0.2  0.56 0.24]: 0.794\n",
      "Testing weights: [0.2  0.64 0.16]\n",
      "Mean AUC-ROC for weights [0.2  0.64 0.16]: 0.789\n",
      "Testing weights: [0.2  0.72 0.08]\n",
      "Mean AUC-ROC for weights [0.2  0.72 0.08]: 0.781\n",
      "Testing weights: [0.2 0.8 0. ]\n",
      "Mean AUC-ROC for weights [0.2 0.8 0. ]: 0.773\n",
      "Testing weights: [0.3 0.  0.7]\n",
      "Mean AUC-ROC for weights [0.3 0.  0.7]: 0.814\n",
      "Testing weights: [0.3  0.07 0.63]\n",
      "Mean AUC-ROC for weights [0.3  0.07 0.63]: 0.821\n",
      "New best configuration found! Previous best: 0.819\n",
      "Testing weights: [0.3  0.14 0.56]\n",
      "Mean AUC-ROC for weights [0.3  0.14 0.56]: 0.825\n",
      "New best configuration found! Previous best: 0.821\n",
      "Testing weights: [0.3  0.21 0.49]\n",
      "Mean AUC-ROC for weights [0.3  0.21 0.49]: 0.826\n",
      "New best configuration found! Previous best: 0.825\n",
      "Testing weights: [0.3  0.28 0.42]\n",
      "Mean AUC-ROC for weights [0.3  0.28 0.42]: 0.824\n",
      "Testing weights: [0.3  0.35 0.35]\n",
      "Mean AUC-ROC for weights [0.3  0.35 0.35]: 0.819\n",
      "Testing weights: [0.3  0.42 0.28]\n",
      "Mean AUC-ROC for weights [0.3  0.42 0.28]: 0.805\n",
      "Testing weights: [0.3  0.49 0.21]\n",
      "Mean AUC-ROC for weights [0.3  0.49 0.21]: 0.798\n",
      "Testing weights: [0.3  0.56 0.14]\n",
      "Mean AUC-ROC for weights [0.3  0.56 0.14]: 0.792\n",
      "Testing weights: [0.3  0.63 0.07]\n",
      "Mean AUC-ROC for weights [0.3  0.63 0.07]: 0.785\n",
      "Testing weights: [0.3 0.7 0. ]\n",
      "Mean AUC-ROC for weights [0.3 0.7 0. ]: 0.776\n",
      "Testing weights: [0.4 0.  0.6]\n",
      "Mean AUC-ROC for weights [0.4 0.  0.6]: 0.820\n",
      "Testing weights: [0.4  0.06 0.54]\n",
      "Mean AUC-ROC for weights [0.4  0.06 0.54]: 0.826\n",
      "New best configuration found! Previous best: 0.826\n",
      "Testing weights: [0.4  0.12 0.48]\n",
      "Mean AUC-ROC for weights [0.4  0.12 0.48]: 0.829\n",
      "New best configuration found! Previous best: 0.826\n",
      "Testing weights: [0.4  0.18 0.42]\n",
      "Mean AUC-ROC for weights [0.4  0.18 0.42]: 0.829\n",
      "Testing weights: [0.4  0.24 0.36]\n",
      "Mean AUC-ROC for weights [0.4  0.24 0.36]: 0.827\n",
      "Testing weights: [0.4 0.3 0.3]\n",
      "Mean AUC-ROC for weights [0.4 0.3 0.3]: 0.822\n",
      "Testing weights: [0.4  0.36 0.24]\n",
      "Mean AUC-ROC for weights [0.4  0.36 0.24]: 0.818\n",
      "Testing weights: [0.4  0.42 0.18]\n",
      "Mean AUC-ROC for weights [0.4  0.42 0.18]: 0.809\n",
      "Testing weights: [0.4  0.48 0.12]\n",
      "Mean AUC-ROC for weights [0.4  0.48 0.12]: 0.795\n",
      "Testing weights: [0.4  0.54 0.06]\n",
      "Mean AUC-ROC for weights [0.4  0.54 0.06]: 0.788\n",
      "Testing weights: [0.4 0.6 0. ]\n",
      "Mean AUC-ROC for weights [0.4 0.6 0. ]: 0.781\n",
      "Testing weights: [0.5 0.  0.5]\n",
      "Mean AUC-ROC for weights [0.5 0.  0.5]: 0.824\n",
      "Testing weights: [0.5  0.05 0.45]\n",
      "Mean AUC-ROC for weights [0.5  0.05 0.45]: 0.827\n",
      "Testing weights: [0.5 0.1 0.4]\n",
      "Mean AUC-ROC for weights [0.5 0.1 0.4]: 0.829\n",
      "New best configuration found! Previous best: 0.829\n",
      "Testing weights: [0.5  0.15 0.35]\n",
      "Mean AUC-ROC for weights [0.5  0.15 0.35]: 0.828\n",
      "Testing weights: [0.5 0.2 0.3]\n",
      "Mean AUC-ROC for weights [0.5 0.2 0.3]: 0.827\n",
      "Testing weights: [0.5  0.25 0.25]\n",
      "Mean AUC-ROC for weights [0.5  0.25 0.25]: 0.822\n",
      "Testing weights: [0.5 0.3 0.2]\n",
      "Mean AUC-ROC for weights [0.5 0.3 0.2]: 0.818\n",
      "Testing weights: [0.5  0.35 0.15]\n",
      "Mean AUC-ROC for weights [0.5  0.35 0.15]: 0.814\n",
      "Testing weights: [0.5 0.4 0.1]\n",
      "Mean AUC-ROC for weights [0.5 0.4 0.1]: 0.807\n",
      "Testing weights: [0.5  0.45 0.05]\n",
      "Mean AUC-ROC for weights [0.5  0.45 0.05]: 0.797\n",
      "Testing weights: [0.5 0.5 0. ]\n",
      "Mean AUC-ROC for weights [0.5 0.5 0. ]: 0.784\n",
      "Testing weights: [0.6 0.  0.4]\n",
      "Mean AUC-ROC for weights [0.6 0.  0.4]: 0.823\n",
      "Testing weights: [0.6  0.04 0.36]\n",
      "Mean AUC-ROC for weights [0.6  0.04 0.36]: 0.826\n",
      "Testing weights: [0.6  0.08 0.32]\n",
      "Mean AUC-ROC for weights [0.6  0.08 0.32]: 0.827\n",
      "Testing weights: [0.6  0.12 0.28]\n",
      "Mean AUC-ROC for weights [0.6  0.12 0.28]: 0.826\n",
      "Testing weights: [0.6  0.16 0.24]\n",
      "Mean AUC-ROC for weights [0.6  0.16 0.24]: 0.823\n",
      "Testing weights: [0.6 0.2 0.2]\n",
      "Mean AUC-ROC for weights [0.6 0.2 0.2]: 0.819\n",
      "Testing weights: [0.6  0.24 0.16]\n",
      "Mean AUC-ROC for weights [0.6  0.24 0.16]: 0.815\n",
      "Testing weights: [0.6  0.28 0.12]\n",
      "Mean AUC-ROC for weights [0.6  0.28 0.12]: 0.812\n",
      "Testing weights: [0.6  0.32 0.08]\n",
      "Mean AUC-ROC for weights [0.6  0.32 0.08]: 0.807\n",
      "Testing weights: [0.6  0.36 0.04]\n",
      "Mean AUC-ROC for weights [0.6  0.36 0.04]: 0.801\n",
      "Testing weights: [0.6 0.4 0. ]\n",
      "Mean AUC-ROC for weights [0.6 0.4 0. ]: 0.795\n",
      "Testing weights: [0.7 0.  0.3]\n",
      "Mean AUC-ROC for weights [0.7 0.  0.3]: 0.819\n",
      "Testing weights: [0.7  0.03 0.27]\n",
      "Mean AUC-ROC for weights [0.7  0.03 0.27]: 0.821\n",
      "Testing weights: [0.7  0.06 0.24]\n",
      "Mean AUC-ROC for weights [0.7  0.06 0.24]: 0.821\n",
      "Testing weights: [0.7  0.09 0.21]\n",
      "Mean AUC-ROC for weights [0.7  0.09 0.21]: 0.821\n",
      "Testing weights: [0.7  0.12 0.18]\n",
      "Mean AUC-ROC for weights [0.7  0.12 0.18]: 0.817\n",
      "Testing weights: [0.7  0.15 0.15]\n",
      "Mean AUC-ROC for weights [0.7  0.15 0.15]: 0.813\n",
      "Testing weights: [0.7  0.18 0.12]\n",
      "Mean AUC-ROC for weights [0.7  0.18 0.12]: 0.810\n",
      "Testing weights: [0.7  0.21 0.09]\n",
      "Mean AUC-ROC for weights [0.7  0.21 0.09]: 0.806\n",
      "Testing weights: [0.7  0.24 0.06]\n",
      "Mean AUC-ROC for weights [0.7  0.24 0.06]: 0.803\n",
      "Testing weights: [0.7  0.27 0.03]\n",
      "Mean AUC-ROC for weights [0.7  0.27 0.03]: 0.799\n",
      "Testing weights: [0.7 0.3 0. ]\n",
      "Mean AUC-ROC for weights [0.7 0.3 0. ]: 0.794\n",
      "Testing weights: [0.8 0.  0.2]\n",
      "Mean AUC-ROC for weights [0.8 0.  0.2]: 0.811\n",
      "Testing weights: [0.8  0.02 0.18]\n",
      "Mean AUC-ROC for weights [0.8  0.02 0.18]: 0.812\n",
      "Testing weights: [0.8  0.04 0.16]\n",
      "Mean AUC-ROC for weights [0.8  0.04 0.16]: 0.811\n",
      "Testing weights: [0.8  0.06 0.14]\n",
      "Mean AUC-ROC for weights [0.8  0.06 0.14]: 0.811\n",
      "Testing weights: [0.8  0.08 0.12]\n",
      "Mean AUC-ROC for weights [0.8  0.08 0.12]: 0.809\n",
      "Testing weights: [0.8 0.1 0.1]\n",
      "Mean AUC-ROC for weights [0.8 0.1 0.1]: 0.805\n",
      "Testing weights: [0.8  0.12 0.08]\n",
      "Mean AUC-ROC for weights [0.8  0.12 0.08]: 0.803\n",
      "Testing weights: [0.8  0.14 0.06]\n",
      "Mean AUC-ROC for weights [0.8  0.14 0.06]: 0.799\n",
      "Testing weights: [0.8  0.16 0.04]\n",
      "Mean AUC-ROC for weights [0.8  0.16 0.04]: 0.797\n",
      "Testing weights: [0.8  0.18 0.02]\n",
      "Mean AUC-ROC for weights [0.8  0.18 0.02]: 0.794\n",
      "Testing weights: [0.8 0.2 0. ]\n",
      "Mean AUC-ROC for weights [0.8 0.2 0. ]: 0.789\n",
      "Testing weights: [0.9 0.  0.1]\n",
      "Mean AUC-ROC for weights [0.9 0.  0.1]: 0.799\n",
      "Testing weights: [0.9  0.01 0.09]\n",
      "Mean AUC-ROC for weights [0.9  0.01 0.09]: 0.800\n",
      "Testing weights: [0.9  0.02 0.08]\n",
      "Mean AUC-ROC for weights [0.9  0.02 0.08]: 0.800\n",
      "Testing weights: [0.9  0.03 0.07]\n",
      "Mean AUC-ROC for weights [0.9  0.03 0.07]: 0.799\n",
      "Testing weights: [0.9  0.04 0.06]\n",
      "Mean AUC-ROC for weights [0.9  0.04 0.06]: 0.797\n",
      "Testing weights: [0.9  0.05 0.05]\n",
      "Mean AUC-ROC for weights [0.9  0.05 0.05]: 0.796\n",
      "Testing weights: [0.9  0.06 0.04]\n",
      "Mean AUC-ROC for weights [0.9  0.06 0.04]: 0.794\n",
      "Testing weights: [0.9  0.07 0.03]\n",
      "Mean AUC-ROC for weights [0.9  0.07 0.03]: 0.790\n",
      "Testing weights: [0.9  0.08 0.02]\n",
      "Mean AUC-ROC for weights [0.9  0.08 0.02]: 0.789\n",
      "Testing weights: [0.9  0.09 0.01]\n",
      "Mean AUC-ROC for weights [0.9  0.09 0.01]: 0.787\n",
      "Testing weights: [0.9 0.1 0. ]\n",
      "Mean AUC-ROC for weights [0.9 0.1 0. ]: 0.784\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "Testing weights: [1. 0. 0.]\n",
      "Mean AUC-ROC for weights [1. 0. 0.]: 0.771\n",
      "\n",
      "Best combination found:\n",
      "Sub-LOF: 0.500\n",
      "GrammarViz3: 0.100\n",
      "DWT-MLEAD: 0.400\n",
      "Mean AUC-ROC: 0.829\n"
     ]
    }
   ],
   "source": [
    "algorithms = [\"Sub-LOF\", \"GrammarViz3\", \"DWT-MLEAD\"]\n",
    "all_results = combine_algorithms(algorithms, family='all', n_weight_points=11)\n",
    "print(\"\\nBest combination found:\")\n",
    "for algo, weight in zip(algorithms, all_results['weights']):\n",
    "    print(f\"{algo}: {weight:.3f}\")\n",
    "print(f\"Mean AUC-ROC: {all_results['mean_auc_roc']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
